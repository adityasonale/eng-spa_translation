{"cells":[{"cell_type":"markdown","metadata":{},"source":["Importing Libraries"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:03:02.070110Z","iopub.status.busy":"2023-11-01T09:03:02.069773Z","iopub.status.idle":"2023-11-01T09:03:11.929039Z","shell.execute_reply":"2023-11-01T09:03:11.928030Z","shell.execute_reply.started":"2023-11-01T09:03:02.070082Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import nltk\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense,LSTM,Input,Attention,TimeDistributed,Dot,Activation\n","import tensorflow as tf\n","import random"]},{"cell_type":"markdown","metadata":{},"source":["Loading the eng-spanish translation file"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:03:11.931368Z","iopub.status.busy":"2023-11-01T09:03:11.930837Z","iopub.status.idle":"2023-11-01T09:03:12.080361Z","shell.execute_reply":"2023-11-01T09:03:12.079396Z","shell.execute_reply.started":"2023-11-01T09:03:11.931341Z"},"trusted":true},"outputs":[],"source":["# with open(r\"/kaggle/input/eng-spanish/spa.txt\",'r', encoding='utf-8') as file:\n","with open(r\"D:\\Datasets\\spa-eng\\spa.txt\",'r', encoding='utf-8') as file:\n","    lines = file.read().split('\\n')"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:03:12.081920Z","iopub.status.busy":"2023-11-01T09:03:12.081561Z","iopub.status.idle":"2023-11-01T09:03:12.099435Z","shell.execute_reply":"2023-11-01T09:03:12.098273Z","shell.execute_reply.started":"2023-11-01T09:03:12.081885Z"},"trusted":true},"outputs":[],"source":["input_texts = []\n","target_texts = []\n","\n","for line in lines[: min(10000, len(lines) - 1)]:\n","    line = line.split('\\t')\n","    input_texts.append(line[0])\n","    line[1] = '\\t' + line[1] + '\\n'\n","    target_texts.append(line[1])"]},{"cell_type":"markdown","metadata":{},"source":["Creating corpus for input and target characters"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:03:12.102308Z","iopub.status.busy":"2023-11-01T09:03:12.101752Z","iopub.status.idle":"2023-11-01T09:03:12.148351Z","shell.execute_reply":"2023-11-01T09:03:12.147650Z","shell.execute_reply.started":"2023-11-01T09:03:12.102280Z"},"trusted":true},"outputs":[],"source":["input_characters = set()\n","target_characters = set()\n","\n","for sentence in input_texts:\n","    for char in sentence:\n","        if char not in input_characters:\n","            input_characters.add(char)\n","\n","for sentence in target_texts:\n","    for char in sentence:\n","        if char not in target_characters:\n","            target_characters.add(char)\n","\n","input_characters = sorted(input_characters)\n","target_characters = sorted(target_characters)\n","\n","\n","len_input_tokens = len(input_characters)\n","len_target_tokens = len(target_characters)"]},{"cell_type":"markdown","metadata":{},"source":["Finding Maximum Sentence Length"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:03:12.149560Z","iopub.status.busy":"2023-11-01T09:03:12.149287Z","iopub.status.idle":"2023-11-01T09:03:12.157135Z","shell.execute_reply":"2023-11-01T09:03:12.156179Z","shell.execute_reply.started":"2023-11-01T09:03:12.149514Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["17\n","42\n"]}],"source":["max_input_sentence_length = max([len(sent) for sent in input_texts])\n","max_target_sentence_length = max([len(sent) for sent in target_texts])\n","\n","print(max_input_sentence_length)\n","print(max_target_sentence_length)\n","\n","total_input_sentence = len(input_texts)\n","total_target_sentence = len(target_texts)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:03:12.158590Z","iopub.status.busy":"2023-11-01T09:03:12.158225Z","iopub.status.idle":"2023-11-01T09:03:12.168070Z","shell.execute_reply":"2023-11-01T09:03:12.167201Z","shell.execute_reply.started":"2023-11-01T09:03:12.158554Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total input Tokens ::  71\n","Total target Tokens ::  86\n","max input length ::  17\n","max target length ::  42\n"]}],"source":["print(\"Total input Tokens :: \",len_input_tokens)\n","print(\"Total target Tokens :: \",len_target_tokens)\n","print(\"max input length :: \",max_input_sentence_length)\n","print(\"max target length :: \",max_target_sentence_length)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:03:12.169260Z","iopub.status.busy":"2023-11-01T09:03:12.169010Z","iopub.status.idle":"2023-11-01T09:03:12.182493Z","shell.execute_reply":"2023-11-01T09:03:12.181757Z","shell.execute_reply.started":"2023-11-01T09:03:12.169238Z"},"trusted":true},"outputs":[],"source":["encoder_input_data = np.zeros((total_input_sentence,max_input_sentence_length,len_input_tokens),dtype='float32')\n","decoder_input_data = np.zeros((total_target_sentence,max_target_sentence_length,len_target_tokens),dtype='float32')\n","decoder_target_data = np.zeros((total_target_sentence,max_target_sentence_length,len_target_tokens),dtype='float32')"]},{"cell_type":"markdown","metadata":{},"source":["Creating a dictionaries for input and target characters"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:03:12.183980Z","iopub.status.busy":"2023-11-01T09:03:12.183647Z","iopub.status.idle":"2023-11-01T09:03:12.192876Z","shell.execute_reply":"2023-11-01T09:03:12.191966Z","shell.execute_reply.started":"2023-11-01T09:03:12.183940Z"},"trusted":true},"outputs":[],"source":["input_token_index = dict((char,i) for i,char in enumerate(input_characters))\n","target_token_index = dict((char,i) for i,char in enumerate(target_characters))"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:03:12.194273Z","iopub.status.busy":"2023-11-01T09:03:12.193959Z","iopub.status.idle":"2023-11-01T09:03:12.733949Z","shell.execute_reply":"2023-11-01T09:03:12.733090Z","shell.execute_reply.started":"2023-11-01T09:03:12.194243Z"},"trusted":true},"outputs":[],"source":["for i,(input_text,target_text) in enumerate(zip(input_texts,target_texts)):\n","    for t,char in enumerate(input_text):\n","        encoder_input_data[i,t,input_token_index[char]] = 1\n","    encoder_input_data[i,t+1:,input_token_index[' ']] = 1\n","    for t, char in enumerate(target_text):\n","        decoder_input_data[i,t,target_token_index[char]] = 1\n","        if t > 0:\n","            decoder_target_data[i,t-1,target_token_index[char]] = 1\n","    decoder_input_data[i,t+1:,target_token_index[' ']] = 1\n","    decoder_target_data[i,t:,target_token_index[' ']] = 1"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:03:12.738227Z","iopub.status.busy":"2023-11-01T09:03:12.737921Z","iopub.status.idle":"2023-11-01T09:03:12.901375Z","shell.execute_reply":"2023-11-01T09:03:12.900387Z","shell.execute_reply.started":"2023-11-01T09:03:12.738203Z"},"trusted":true},"outputs":[],"source":["combined = list(zip(encoder_input_data,decoder_input_data,decoder_target_data))\n","\n","random.shuffle(combined)\n","\n","# Split the shuffled data back into separate arrays\n","encoder_input_data, decoder_input_data, decoder_target_data = zip(*combined)\n","\n","# Convert the arrays back to NumPy arrays if needed\n","encoder_input_data = np.array(encoder_input_data)\n","decoder_input_data = np.array(decoder_input_data)\n","decoder_target_data = np.array(decoder_target_data)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["(10000, 42, 86)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["decoder_target_data.shape"]},{"cell_type":"markdown","metadata":{},"source":["Creating Encoder Decoder Model with Attention"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:06:08.323637Z","iopub.status.busy":"2023-11-01T09:06:08.322625Z","iopub.status.idle":"2023-11-01T09:06:08.869992Z","shell.execute_reply":"2023-11-01T09:06:08.869174Z","shell.execute_reply.started":"2023-11-01T09:06:08.323593Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["KerasTensor(type_spec=TensorSpec(shape=(None, None, 512), dtype=tf.float32, name=None), name='Encoder/PartitionedCall:1', description=\"created by layer 'Encoder'\")\n"]}],"source":["# Encoder\n","encoder_inputs = Input(shape=(None,len_input_tokens),name=\"Encoder Input\")\n","\n","encoder_lstm = LSTM(512,return_sequences=True,return_state=True, name=\"Encoder\") # LSTM Layer\n","\n","encoder_output,state_h,state_c = encoder_lstm(encoder_inputs)\n","\n","print(encoder_output)\n","\n","encoder_states = [state_h,state_c]\n","\n","# Decoder\n","decoder_inputs = Input(shape=(None,len_target_tokens),name=\"Decoder Input\")\n","\n","decoder_lstm = LSTM(512, return_sequences= True, return_state=True, name=\"Decoder\")\n","\n","decoder_output,_,_ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n","\n","attention = Attention()([decoder_output,encoder_output])\n","\n","outputs = tf.concat([decoder_output,attention],axis=-1)\n","\n","# decoder_lstm_output_dropout = Dropout(0.5)(outputs)\n","\n","# Dense Layer\n","decoder_dense = Dense(len_target_tokens,activation='softmax', name=\"Decoder_Dense_Layer\")\n","\n","dense_time = TimeDistributed(decoder_dense,name=\"final_layer\")\n","\n","outputs = dense_time(outputs)"]},{"cell_type":"markdown","metadata":{},"source":["Adding Custom Attention mechanism into the model"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(None, None, None)\n"]}],"source":["# Encoder\n","encoder_inputs = Input(shape=(None,len_input_tokens),name=\"Encoder Input\")\n","\n","encoder_lstm = LSTM(512,return_sequences=True,return_state=True, name=\"Encoder\") # LSTM Layer\n","\n","encoder_output,state_h,state_c = encoder_lstm(encoder_inputs)\n","\n","encoder_states = [state_h,state_c]\n","\n","# Decoder\n","decoder_inputs = Input(shape=(None,len_target_tokens),name=\"Decoder Input\")\n","\n","decoder_lstm = LSTM(512, return_sequences= True, return_state=True, name=\"Decoder\")\n","\n","decoder_output,_,_ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n","\n","# Custom attention mechanism\n","\n","dot_layer = Dot(axes=(2,2))([decoder_output,encoder_output])\n","\n","attention_layer = Activation('softmax')(dot_layer)\n","\n","attention_vec = Dot(axes=(2,1))([attention_layer,encoder_output])\n","\n","print(attention.shape)\n","\n","#-------------------------------------------------------------------------------------------------------------------\n","\n","outputs = tf.concat([decoder_output,attention_vec],axis=-1)\n","\n","# decoder_lstm_output_dropout = Dropout(0.5)(outputs)\n","\n","# Dense Layer\n","decoder_dense = Dense(len_target_tokens,activation='softmax', name=\"Decoder_Dense_Layer\")\n","\n","dense_time = TimeDistributed(decoder_dense,name=\"final_layer\")\n","\n","outputs = dense_time(outputs)"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:03:17.836561Z","iopub.status.busy":"2023-11-01T09:03:17.836224Z","iopub.status.idle":"2023-11-01T09:03:17.847712Z","shell.execute_reply":"2023-11-01T09:03:17.846794Z","shell.execute_reply.started":"2023-11-01T09:03:17.836511Z"},"trusted":true},"outputs":[],"source":["model = Model([encoder_inputs,decoder_inputs],outputs)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:03:17.849205Z","iopub.status.busy":"2023-11-01T09:03:17.848864Z","iopub.status.idle":"2023-11-01T09:03:17.883588Z","shell.execute_reply":"2023-11-01T09:03:17.882769Z","shell.execute_reply.started":"2023-11-01T09:03:17.849180Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," Encoder Input (InputLayer)  [(None, None, 71)]           0         []                            \n","                                                                                                  \n"," Decoder Input (InputLayer)  [(None, None, 86)]           0         []                            \n","                                                                                                  \n"," Encoder (LSTM)              [(None, None, 512),          1196032   ['Encoder Input[0][0]']       \n","                              (None, 512),                                                        \n","                              (None, 512)]                                                        \n","                                                                                                  \n"," Decoder (LSTM)              [(None, None, 512),          1226752   ['Decoder Input[0][0]',       \n","                              (None, 512),                           'Encoder[0][1]',             \n","                              (None, 512)]                           'Encoder[0][2]']             \n","                                                                                                  \n"," attention (Attention)       (None, None, 512)            0         ['Decoder[0][0]',             \n","                                                                     'Encoder[0][0]']             \n","                                                                                                  \n"," tf.concat (TFOpLambda)      (None, None, 1024)           0         ['Decoder[0][0]',             \n","                                                                     'attention[0][0]']           \n","                                                                                                  \n"," final_layer (TimeDistribut  (None, None, 86)             88150     ['tf.concat[0][0]']           \n"," ed)                                                                                              \n","                                                                                                  \n","==================================================================================================\n","Total params: 2510934 (9.58 MB)\n","Trainable params: 2510934 (9.58 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:03:17.885007Z","iopub.status.busy":"2023-11-01T09:03:17.884705Z","iopub.status.idle":"2023-11-01T09:05:22.591726Z","shell.execute_reply":"2023-11-01T09:05:22.590713Z","shell.execute_reply.started":"2023-11-01T09:03:17.884983Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/60\n","125/125 [==============================] - 120s 946ms/step - loss: 1.5457 - accuracy: 0.6370 - val_loss: 1.2666 - val_accuracy: 0.6545\n","Epoch 2/60\n","125/125 [==============================] - 153s 1s/step - loss: 1.2422 - accuracy: 0.6651 - val_loss: 1.1258 - val_accuracy: 0.6931\n","Epoch 3/60\n"," 30/125 [======>.......................] - ETA: 2:03 - loss: 1.1038 - accuracy: 0.6995"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32md:\\vs code\\python\\DeepLearning\\Projects\\eng_spanish_translation\\seq2seq-attention-eng2-spa.ipynb Cell 23\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/vs%20code/python/DeepLearning/Projects/eng_spanish_translation/seq2seq-attention-eng2-spa.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/vs%20code/python/DeepLearning/Projects/eng_spanish_translation/seq2seq-attention-eng2-spa.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit([encoder_input_data,decoder_input_data],decoder_target_data,batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m,epochs\u001b[39m=\u001b[39;49m\u001b[39m60\u001b[39;49m,validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n","File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n","File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n","File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n","File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n","File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n","File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n","File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n","File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n","File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","model.fit([encoder_input_data,decoder_input_data],decoder_target_data,batch_size=64,epochs=60,validation_split=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:05:22.593356Z","iopub.status.busy":"2023-11-01T09:05:22.592976Z","iopub.status.idle":"2023-11-01T09:05:22.645963Z","shell.execute_reply":"2023-11-01T09:05:22.645003Z","shell.execute_reply.started":"2023-11-01T09:05:22.593323Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}],"source":["model.save(\"/kaggle/working/eng_spa.h5\")"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:05:22.647427Z","iopub.status.busy":"2023-11-01T09:05:22.647137Z","iopub.status.idle":"2023-11-01T09:05:23.267491Z","shell.execute_reply":"2023-11-01T09:05:23.266460Z","shell.execute_reply.started":"2023-11-01T09:05:22.647402Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","\n","saved_model = load_model(r\"D:\\vs code\\python\\DeepLearning\\Projects\\eng_spanish_translation\\eng_spa2.h5\")\n","# saved_model = load_model(\"/kaggle/working/eng_spa.h5\")\n"]},{"cell_type":"markdown","metadata":{},"source":["Creating Custom Encoder Decoder Model"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:05:23.269766Z","iopub.status.busy":"2023-11-01T09:05:23.268886Z","iopub.status.idle":"2023-11-01T09:05:23.299890Z","shell.execute_reply":"2023-11-01T09:05:23.297490Z","shell.execute_reply.started":"2023-11-01T09:05:23.269728Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<keras.src.engine.input_layer.InputLayer object at 0x0000016F7F2CF350>\n","<keras.src.engine.input_layer.InputLayer object at 0x0000016F7B6D5A90>\n","<keras.src.layers.rnn.lstm.LSTM object at 0x0000016F7F378790>\n","<keras.src.layers.rnn.lstm.LSTM object at 0x0000016F7F3A4950>\n","<keras.src.layers.merging.dot.Dot object at 0x0000016F7F332110>\n","<keras.src.layers.core.activation.Activation object at 0x0000016F798D4B50>\n","<keras.src.layers.merging.dot.Dot object at 0x0000016F7F34F990>\n","<keras.src.layers.core.tf_op_layer.TFOpLambda object at 0x0000016F7F3A3550>\n","<keras.src.layers.regularization.dropout.Dropout object at 0x0000016F7F471B10>\n","<keras.src.layers.rnn.time_distributed.TimeDistributed object at 0x0000016F7F3326D0>\n"]}],"source":["for layer in saved_model.layers:\n","    print(layer)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_9\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," Encoder Input (InputLayer)  [(None, None, 71)]           0         []                            \n","                                                                                                  \n"," Decoder Input (InputLayer)  [(None, None, 86)]           0         []                            \n","                                                                                                  \n"," Encoder (LSTM)              [(None, None, 256),          335872    ['Encoder Input[0][0]']       \n","                              (None, 256),                                                        \n","                              (None, 256)]                                                        \n","                                                                                                  \n"," Decoder (LSTM)              [(None, None, 256),          351232    ['Decoder Input[0][0]',       \n","                              (None, 256),                           'Encoder[0][1]',             \n","                              (None, 256)]                           'Encoder[0][2]']             \n","                                                                                                  \n"," dot_14 (Dot)                (None, None, None)           0         ['Decoder[0][0]',             \n","                                                                     'Encoder[0][0]']             \n","                                                                                                  \n"," activation_7 (Activation)   (None, None, None)           0         ['dot_14[0][0]']              \n","                                                                                                  \n"," dot_15 (Dot)                (None, None, 256)            0         ['activation_7[0][0]',        \n","                                                                     'Encoder[0][0]']             \n","                                                                                                  \n"," tf.concat_7 (TFOpLambda)    (None, None, 512)            0         ['Decoder[0][0]',             \n","                                                                     'dot_15[0][0]']              \n","                                                                                                  \n"," dropout_3 (Dropout)         (None, None, 512)            0         ['tf.concat_7[0][0]']         \n","                                                                                                  \n"," final_layer (TimeDistribut  (None, None, 86)             44118     ['dropout_3[0][0]']           \n"," ed)                                                                                              \n","                                                                                                  \n","==================================================================================================\n","Total params: 731222 (2.79 MB)\n","Trainable params: 731222 (2.79 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}],"source":["saved_model.summary()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:07:07.462570Z","iopub.status.busy":"2023-11-01T09:07:07.462159Z","iopub.status.idle":"2023-11-01T09:07:07.963889Z","shell.execute_reply":"2023-11-01T09:07:07.963098Z","shell.execute_reply.started":"2023-11-01T09:07:07.462538Z"},"trusted":true},"outputs":[],"source":["# # Encoder\n","# encoder_input = saved_model.layers[0].input\n","# encoder_lstm = saved_model.layers[2]\n","# encoder_output,state_h,state_c = encoder_lstm(encoder_input)\n","# encoder_states = [state_h,state_c]\n","\n","# encoder_model = Model(encoder_input,[encoder_output,encoder_states])\n","\n","# # Decoder\n","\n","# decoder_initial_state_h = Input(shape=(512,))\n","# decoder_initial_state_c = Input(shape=(512,))\n","# encoder_output_2 = Input(shape=(512,))\n","\n","# decoder_initial_states = [decoder_initial_state_h,decoder_initial_state_c]\n","\n","# decoder_input = saved_model.layers[1].input\n","# decoder_lstm = saved_model.layers[3]\n","# decoder_output,state_h,state_c = decoder_lstm(decoder_input, initial_state=decoder_initial_states)\n","\n","# decoder_states=[state_h,state_c]\n","\n","# attention_layer = saved_model.layers[4]\n","\n","# # attention = attention_layer\n","# attention = attention_layer([decoder_output,encoder_output_2])\n","\n","# concatenation_layer = saved_model.layers[5]\n","\n","# outputs = concatenation_layer([decoder_output,attention],axis=-1)\n","\n","# dense_layer = saved_model.layers[6]\n","\n","# outputs = dense_layer(outputs)\n","\n","# decoder_model = Model([decoder_input,encoder_output_2,decoder_initial_states],[outputs] + decoder_states)"]},{"cell_type":"markdown","metadata":{},"source":["Implementing Attention on Inference Model"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["# Encoder\n","\n","encoder_inputs = saved_model.layers[0].input\n","encoder_output,state_h,state_c = saved_model.layers[2](encoder_inputs)  # encoder lstm\n","encoder_states = [state_h,state_c]  # encoder states\n","\n","encoder_model = Model(encoder_inputs,[encoder_output,encoder_states])\n","\n","# decoder\n","\n","decoder_state_h = Input(shape=(256,))\n","decoder_state_c = Input(shape=(256,))\n","\n","decoder_initial_states = [decoder_state_h,decoder_state_c]\n","\n","decoder_inputs = saved_model.layers[1].input\n","decoder_output,decoder_state_h,decoder_state_c = saved_model.layers[3](decoder_inputs,initial_state=decoder_initial_states)  # decoder lstm\n","decoder_states = [decoder_state_h,decoder_state_c]  # decoder states\n","\n","# Implementing Attention\n","\n","dot_product = saved_model.layers[4]([decoder_output,encoder_output])\n","\n","attention = saved_model.layers[5](dot_product)\n","\n","attention_vec = saved_model.layers[6]([attention,encoder_output])\n","\n","context_vector = saved_model.layers[7]([decoder_output,attention_vec],axis=-1)\n","\n","dropout = saved_model.layers[8](context_vector)\n","\n","outputs = saved_model.layers[9](dropout)"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[],"source":["decoder_model = Model([decoder_inputs,decoder_initial_states,encoder_output],[outputs,decoder_states])"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2023-11-01T09:07:15.362724Z","iopub.status.busy":"2023-11-01T09:07:15.361722Z","iopub.status.idle":"2023-11-01T09:07:15.367387Z","shell.execute_reply":"2023-11-01T09:07:15.366232Z","shell.execute_reply.started":"2023-11-01T09:07:15.362679Z"},"trusted":true},"outputs":[],"source":["reverse_target_index = dict((i,char) for char,i in target_token_index.items())"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 52ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 28ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 20ms/step\n","-\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 26ms/step\n","-\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 19ms/step\n","-\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","-\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 23ms/step\n","-\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 25ms/step\n","-\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","-\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 15ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 22ms/step\n","-\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 23ms/step\n","-\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 25ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","-\n"]}],"source":["def decode(sentence):\n","#     sentence_encoded = np.zeros((1,max_input_length,len_input_tokens))\n","    \n","# # one hot encoding the input sentence\n","#     for i, char in enumerate(sentence):\n","#         sentence_encoded[0,i,input_token_index[char]] = 1\n","#     sentence_encoded[0,i+1:,input_token_index[' ']] = 1\n","    \n","# predicting the encoder states\n","    encoder_output,states = encoder_model.predict(sentence)\n","    decoded_sentence = \"\"\n","    decoded_input = np.zeros((1,1,len_target_tokens))\n","    decoded_input[0,0,target_token_index['\\t']] = 1\n","    sample_character = \"\"\n","    stop_condition = False\n","    while not stop_condition:\n","        # output,state_h,state_c = decoder_model.predict([decoded_input,states,encoder_output])\n","        output,state_n = decoder_model.predict([decoded_input,states,encoder_output])\n","        token_index = np.argmax(output)\n","        sample_character = reverse_target_index[token_index]\n","        decoded_sentence += sample_character\n","        if sample_character == '\\n' or len(decoded_sentence) > max_target_sentence_length:\n","            stop_condition = True\n","        decoded_input = np.zeros((1,1,len_target_tokens))\n","        decoded_input[0,0,token_index] = 1\n","        states = state_n\n","    return decoded_sentence\n","\n","\n","op = []\n","\n","for seq_index in range(10):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = encoder_input_data[seq_index: seq_index + 1]\n","    decoded_sentence = decode(input_seq)\n","    print('-')\n","    # print('Input sentence:', input_texts[seq_index])\n","    # print('Decoded sentence:', decoded_sentence)\n","    op.append([input_texts[seq_index],decoded_sentence])"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['Go.', 'Vete.\\n']\n","['Go.', 'Vete.\\n']\n","['Go.', 'Vete.\\n']\n","['Go.', 'Vete.\\n']\n","['Hi.', 'Hola.\\n']\n","['Run!', '¡Corre!\\n']\n","['Run.', 'Corred.\\n']\n","['Who?', '¿Quieá el golpao?\\n']\n","['Fire!', '¡Incendio!\\n']\n","['Fire!', '¡Incendio!\\n']\n"]}],"source":["for i in op:\n","    print(i)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
